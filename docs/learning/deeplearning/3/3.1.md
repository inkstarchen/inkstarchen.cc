# 线性回归
在机器学习领域中的大多数任务通常都与预测(prediction)有关.而**回归问题**便是预测的一个种类.

## 线性回归的基本元素
> 线性回归在回归的各种标准工具中最简单而且最流行.

线性回归基于几个简单的假设:

- 假设自变量x和因变量y之间的关系是线性的，即y可以表示为x中元素的加权和.
- 假设任何噪声都比较正常，如噪声遵循正态分布.

举一个实际的例子:为了根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格，我们要开发一个能预测房价的模型.

- 我们需要收集一个真实的数据集.在机器学习的术语中，该数据集被称为**训练数据集（training data set）**或**训练集（taining set）**
- 每行数据称为**样本（sample）**,也可以称为**数据点（data point）**或**数据样本（data instance）**
- 我们把试图预测的目标称为**标签（label）**或**目标（target）**.
- 预测所依据的自变量称为**特征（feature）**或**协变量（covariate）**.

通常用n来表示数据集中的样本数.对索引为$i$的样本，其输入表示为$x^{(i)}= {[ x_1^{(i)}, x_2^{(i)}]}^T$,其对应的标签是$y^{(i)}$.

### 线性模型
根据线性假设我们有：
$$ price = w_area {\cdot} area + w_age {\cdot} age + b$$
其中的$w_area$和$w_age$称为**权重（weight）**,b称为**偏置（bias）**、**偏移量（offset）**或**截距（intercept）**.

给定一个数据集，我们的目标就是确定模型的权重$w和偏置b$.

对于特征集合$X$,预测值$\hat{y} \in R^n$,可以用矩阵-向量乘法表示为：
$$ \hat{y} = Xw + b$$

求和过程将使用广播机制.

考虑观测误差带来的影响，我们引入一个噪声项.

再寻找最好的模型参数（model parameters）$w和b$之前，我们还需要
- 一种模型质量的度量方式；
- 一种能够更新模型以提高模型预测质量的方法

### 损失函数
- 通常选择非负数作为损失
- 回归问题中最常用的损失函数是平方误差函数

当样本$i$的预测值为$\hat{y}^{(i)}$，真实值为$y^{(i)}$时，平方误差可以定义为以下公式：

$$ l^{(i)}(w,b) = \frac{1}{2}(\hat{y}^{(i)} - y^{(i)})^2$$

其中常数$\frac{1}{2}$不会带来本质的差别.

为了度量模型在整个数据集上的质量，我们需计算在训练集n个样本上的损失均值（也等价于求和）

$$L(w,b)=\frac{1}{n}\sum_{i=1}^{n}l^{(i)}(w,b) = \frac{1}{n}\sum_{i=1}^{n}(w^Tx^{(i)} - y^{(i)})^2$$

在训练模型时，我们希望寻找一组参数$(w^*,b^*)$,这组参数能最小化在所有训练样本上的总损失。

$$w^*,b^* = argmin_{w,b}L(w,b)$$

### 解析解
线性回归的解可以用一个公式简单地表达出来，这类解叫做解析解(analytical solution).将损失关于w的导数设为0，得到解析解：

$$w^* = (X^TX)^{-1}X^Ty$$

### 随机梯度下降

梯度下降(gradient descent)最简单的用法是计算损失函数关于模型参数的导数。

为了执行的效率，我们通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做**小批量随机梯度下降(minibatch stochastic gradient descent)**.

在每次迭代中，我们首先随机抽样一个小批量$\beta$,它是由固定数量的训练样本组成的。

更新过程：

$$(w,b)\gets (w,b) - \frac{\eta}{|\beta|}\sum_{i\in\beta}\partial_{(w,b)}l^{(i)}(w,b)$$

算法步骤如下:

- 初始化模型参数的值，如随机初始化；
- 从数据集中随机抽取小批量样本且在负梯度的方向上更新参数,并不断迭代.

$$w\gets w - \frac{\eta}{|\beta|}\sum_{i\in\beta}\partial_{w}l^{(i)}(w,b) = w - \frac{\eta}{|\beta|}\sum_{i\in\beta}x^{(i)}(w^Tx^{(i)} + b- y^{(i)})$$

$$b\gets b - \frac{\eta}{|\beta|}\sum_{i\in\beta}\partial_{b}l^{(i)}(w,b) = b - \frac{\eta}{|\beta|}\sum_{i\in\beta}(w^Tx^{(i)} + b- y^{(i)})$$

其中$|\beta|$表示每个小批量中的样本数，这也称为**批量大小(batch size)**.$\eta$表示**学习率(learning rate)**.批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。

- 这些可以调整但不在训练过程中更新的参数称为**超参数(hyperparameter)**.
- **调参(hyperparameter tuning)**是选择超参数的过程.
- 超参数通常是根据训练迭代结果来调整的，而训练迭代结果是在独立的**验证数据集（validation dataset）**上评估得到的.
- 我们无法达到最小值，会缓慢收敛.

线性回归恰好是一个在整个域中只有一个最小值的学习问题。但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。我们很难找到一组参数使得它能在我们从未见过的数据上实现较低的损失，这一挑战被称为**泛化（generalization）**.

## 矢量化加速
为了同时处理整个小批量的样本，我们需要对计算进行矢量化，从而利用线性代数库，而不是在Python中编写开销高昂的for循环.

## 正态分布与平方损失
**正态分布（normal distribution）**,也称为**高斯分布（Gaussian distribution）**.若随机变量$x$具有均值$\mu$和方差$\sigma^2$，则$x$的正态分布概率密度函数为：

$$p(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$

均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是：我们假设了观测中包含噪声，其中噪声服从正态分布。噪声正态分布如下式：

$$y=w^Tx+b+\epslion$$
其中$\epslion ~ \sim N(0,\sigma^2)$.

然后我们可以写出通过给定$x$观测到特定$y$的**似然（likehood）**:

> **似然函数**:在参数固定的条件下，可以将概率分布理解为关于随机变量的函数：而在观测数据固定的情况下，将概率分布看作关于参数的函数，则称为似然函数.
$$p(y|x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y-w^Tx-b)^2}{2\sigma^2}}$$

根据极大似然估计法，参数$w$和$b$的最优值是使整个数据集的似然最大的值：

$$P(y|X)=\prod_{i=1}^{n}p(y^{(i)}|x^{(i)}).$$

由于历史原因，优化通常是说最小化而不是最大化。我们可以改为**最小化负对数似然**$-logP(y|X)$.

$$-logP(y|X) = \sum_{i=1}^{n}\frac{1}{2}log(2\pi\sigma^2) + \frac{(y^{(i)}-w^Tx^{(i)}-b)^2}{2\sigma^2}$$

接下来假设$\sigma$是某个固定常数.

### 神经网络图
输入为$x_1,\cdots,x_d$,则输入层中的*输入数*（或称为*特征维度*，feature dimensionality)为d.

对于线性回归，每个输入都与每个输出相连，我们将这种变换称为*全连接层*（fully-connected layer）或称为*稠密层*（dense layer）

